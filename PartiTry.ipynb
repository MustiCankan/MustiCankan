{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpXjta4xWQVDCk1Ym3qVrQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MustiCankan/MustiCankan/blob/main/PartiTry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3CBlgbnkM0A"
      },
      "outputs": [],
      "source": [
        "!pip install parti-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First you will need to train your Transformer VQ-GAN VAE"
      ],
      "metadata": {
        "id": "qyvi6pwdkTLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from parti_pytorch import VitVQGanVAE, VQGanVAETrainer\n",
        "\n",
        "vit_vae = VitVQGanVAE(\n",
        "    dim = 256,               # dimensions\n",
        "    image_size = 256,        # target image size\n",
        "    patch_size = 16,         # size of the patches in the image attending to each other\n",
        "    num_layers = 3           # number of layers\n",
        ").cuda()\n",
        "\n",
        "trainer = VQGanVAETrainer(\n",
        "    vit_vae,\n",
        "    folder = '/path/to/your/images',\n",
        "    num_train_steps = 100000,\n",
        "    lr = 3e-4,\n",
        "    batch_size = 4,\n",
        "    grad_accum_every = 8,\n",
        "    amp = True\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "ghCa71CekQOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0rc87R1okWwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from parti_pytorch import Parti, VitVQGanVAE\n",
        "\n",
        "# first instantiate your ViT VQGan VAE\n",
        "# a VQGan VAE made of transformers\n",
        "\n",
        "vit_vae = VitVQGanVAE(\n",
        "    dim = 256,               # dimensions\n",
        "    image_size = 256,        # target image size\n",
        "    patch_size = 16,         # size of the patches in the image attending to each other\n",
        "    num_layers = 3           # number of layers\n",
        ").cuda()\n",
        "\n",
        "vit_vae.load_state_dict(torch.load(f'/path/to/vae.pt')) # you will want to load the exponentially moving averaged VAE\n",
        "\n",
        "# then you plugin the ViT VqGan VAE into your Parti as so\n",
        "\n",
        "parti = Parti(\n",
        "    vae = vit_vae,            # vit vqgan vae\n",
        "    dim = 512,                # model dimension\n",
        "    depth = 8,                # depth\n",
        "    dim_head = 64,            # attention head dimension\n",
        "    heads = 8,                # attention heads\n",
        "    dropout = 0.,             # dropout\n",
        "    cond_drop_prob = 0.25,    # conditional dropout, for classifier free guidance\n",
        "    ff_mult = 4,              # feedforward expansion factor\n",
        "    t5_name = 't5-large',     # name of your T5\n",
        ")\n",
        "\n",
        "# ready your training text and images\n",
        "\n",
        "texts = [\n",
        "    'a child screaming at finding a worm within a half-eaten apple',\n",
        "    'lizard running across the desert on two feet',\n",
        "    'waking up to a psychedelic landscape',\n",
        "    'seashells sparkling in the shallow waters'\n",
        "]\n",
        "\n",
        "images = torch.randn(4, 3, 256, 256).cuda()\n",
        "\n",
        "# feed it into your parti instance, with return_loss set to True\n",
        "\n",
        "loss = parti(\n",
        "    texts = texts,\n",
        "    images = images,\n",
        "    return_loss = True\n",
        ")\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "# do this for a long time on much data\n",
        "# then...\n",
        "\n",
        "images = parti.generate(texts = [\n",
        "    'a whale breaching from afar',\n",
        "    'young girl blowing out candles on her birthday cake',\n",
        "    'fireworks with blue and green sparkles'\n",
        "], cond_scale = 3., return_pil_images = True) # conditioning scale for classifier free guidance\n",
        "\n",
        "# List[PILImages] (256 x 256 RGB)"
      ],
      "metadata": {
        "id": "DJW76RkVkft_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}