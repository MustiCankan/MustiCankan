{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzufulOleYUu68E/+SyJeN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MustiCankan/MustiCankan/blob/main/MinigenTryipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zkp7F47y1ehP"
      },
      "outputs": [],
      "source": [
        "!wget http://images.cocodataset.org/zips/train2017.zip\n",
        "!unzip train2017.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision transformers datasets"
      ],
      "metadata": {
        "id": "eueunzXi1hCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Imagen Training"
      ],
      "metadata": {
        "id": "IOkgFn3i1j6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from datasets import load_dataset\n",
        "from transformers import T5Tokenizer\n",
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Import the MinImagen model (Ensure MinImagen's classes are implemented)\n",
        "from minimagen_model import Imagen, GaussianDiffusion, Unet\n",
        "\n",
        "# **1. Configurations**\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 1e-4\n",
        "IMAGE_SIZE = 64  # Low-res images for faster training\n",
        "DATA_DIR = \"./coco/train2017\"\n",
        "CAPTION_FILE = \"./coco/annotations/captions_train2017.json\"\n",
        "\n",
        "# **2. Load COCO Dataset**\n",
        "def load_coco_data():\n",
        "    # Load captions\n",
        "    with open(CAPTION_FILE, 'r') as f:\n",
        "        captions_data = json.load(f)['annotations']\n",
        "\n",
        "    # Create dictionary with {image_id: caption}\n",
        "    captions_dict = {ann['image_id']: ann['caption'] for ann in captions_data}\n",
        "\n",
        "    image_paths = [os.path.join(DATA_DIR, f) for f in os.listdir(DATA_DIR)]\n",
        "    return captions_dict, image_paths\n",
        "\n",
        "# **3. Custom Dataset Class**\n",
        "class COCODataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, captions_dict, image_paths, tokenizer):\n",
        "        self.captions_dict = captions_dict\n",
        "        self.image_paths = image_paths\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transforms = transforms.Compose([\n",
        "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image_id = int(os.path.basename(img_path).split('.')[0])\n",
        "        caption = self.captions_dict.get(image_id, \"Unknown caption\")\n",
        "\n",
        "        # Load and process image\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        image = self.transforms(image)\n",
        "\n",
        "        # Tokenize caption\n",
        "        tokens = self.tokenizer.encode(caption, return_tensors=\"pt\").squeeze(0)\n",
        "\n",
        "        return tokens, image\n",
        "\n",
        "# **4. Tokenizer and Dataset**\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "captions_dict, image_paths = load_coco_data()\n",
        "dataset = COCODataset(captions_dict, image_paths, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# **5. MinImagen Model Components**\n",
        "unet = Unet(\n",
        "    dim=64,  # Base model dimension\n",
        "    dim_mults=(1, 2, 4),\n",
        "    channels=3\n",
        ")\n",
        "diffusion = GaussianDiffusion(\n",
        "    model=unet,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    timesteps=1000,  # Number of diffusion steps\n",
        "    loss_type=\"l2\"\n",
        ")\n",
        "imagen = Imagen(\n",
        "    unets=(unet,),\n",
        "    text_encoder_name=\"t5-small\"\n",
        ")\n",
        "\n",
        "# **6. Optimizer and Loss Function**\n",
        "optimizer = torch.optim.Adam(imagen.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# **7. Training Loop**\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "imagen.to(device)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    imagen.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, (captions, images) in enumerate(dataloader):\n",
        "        captions, images = captions.to(device), images.to(device)\n",
        "\n",
        "        # Forward pass through the MinImagen model\n",
        "        loss = diffusion(images, captions)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{EPOCHS}], Step [{step}/{len(dataloader)}], Loss: {loss.item()}\")\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}] Loss: {total_loss/len(dataloader)}\")\n",
        "\n",
        "    # Save the model after each epoch\n",
        "    torch.save(imagen.state_dict(), f\"minimagen_epoch_{epoch+1}.pth\")\n",
        "\n",
        "print(\"Training Completed!\")"
      ],
      "metadata": {
        "id": "8wdxKEzK1mym"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}