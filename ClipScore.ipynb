{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPEsJRx6nduxLpsqLlkM7Ls",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MustiCankan/MustiCankan/blob/main/ClipScore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZW50oA2plE1",
        "outputId": "39964571-be25-4079-8963-ccc21def1064"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-1hv07ymx\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-1hv07ymx\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.20.1+cu121)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=9bd5eca54991b1b14fc5b1bffeb7ed923a5fe55bc3b47f614b2b051e563e37a8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lgvnujsp/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e-jHLwCYp677"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLIP Score"
      ],
      "metadata": {
        "id": "2iqg5DnIu3l6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "\n",
        "# Load the CLIP model and preprocess function\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Function to compute CLIP score\n",
        "def compute_clip_score(image_path, text_prompt):\n",
        "    # Load and preprocess the image\n",
        "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
        "\n",
        "    # Encode the image and text\n",
        "    text = clip.tokenize([text_prompt]).to(device)\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image)\n",
        "        text_features = model.encode_text(text)\n",
        "\n",
        "    # Normalize features to unit vectors\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Compute cosine similarity (CLIP score)\n",
        "    similarity = (image_features @ text_features.T).item()\n",
        "\n",
        "    return similarity\n",
        "\n",
        "# Example Usage\n",
        "image_path = \"/content/Unknown-2.png\"  # Path to your image file\n",
        "text_prompt = \"a photograph of an astronaut riding a horse\"  # Your text description\n",
        "clip_score = compute_clip_score(image_path, text_prompt)\n",
        "print(f\"CLIP Score: {clip_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPWW0YBOpryn",
        "outputId": "5238c3fc-766e-472d-9428-4b6ce014cf2f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Score: 0.3706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FID Score Creation"
      ],
      "metadata": {
        "id": "fiFJlg5ZuzqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision scipy numpy pillow tqdm\n",
        "!pip install git+https://github.com/mseitzer/pytorch-fid.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI4ODpwEp2j3",
        "outputId": "9cd490f8-def1-445e-acb2-fbd298ed597d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting git+https://github.com/mseitzer/pytorch-fid.git\n",
            "  Cloning https://github.com/mseitzer/pytorch-fid.git to /tmp/pip-req-build-jngaqsuv\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/mseitzer/pytorch-fid.git /tmp/pip-req-build-jngaqsuv\n",
            "  Resolved https://github.com/mseitzer/pytorch-fid.git to commit b9c18118d082cbd263c1b8963fc4221dc1cbb659\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-fid==0.3.0) (1.26.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pytorch-fid==0.3.0) (11.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pytorch-fid==0.3.0) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-fid==0.3.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-fid==0.3.0) (0.20.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1->pytorch-fid==0.3.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1->pytorch-fid==0.3.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1->pytorch-fid==0.3.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1->pytorch-fid==0.3.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1->pytorch-fid==0.3.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1->pytorch-fid==0.3.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0.1->pytorch-fid==0.3.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.1->pytorch-fid==0.3.0) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pytorch_fid import fid_score\n",
        "\n",
        "# Function to compute FID for text-to-image generation\n",
        "def compute_text_to_image_fid(real_images_dir, generated_images_dir, batch_size=16):\n",
        "    \"\"\"\n",
        "    real_images_dir: Path to directory containing real images corresponding to text prompts.\n",
        "    generated_images_dir: Path to directory containing images generated from text prompts.\n",
        "    \"\"\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Compute FID between real and generated image sets\n",
        "    fid_value = fid_score.calculate_fid_given_paths([real_images_dir, generated_images_dir],\n",
        "    batch_size=batch_size,\n",
        "    device=device,\n",
        "    dims=2048)\n",
        "    print(f\"FID Score (Text-to-Image): {fid_value:.4f}\")\n",
        "    return fid_value\n",
        "\n",
        "# Example Usage\n",
        "real_images_path = \"/content/real\"  # Real images (e.g., from MS COCO)\n",
        "generated_images_path = \"/content/real\"  # Model-generated images\n",
        "fid_score_value = compute_text_to_image_fid(real_images_path, generated_images_path,64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lC8EaogIrjGd",
        "outputId": "c8268607-d176-4b6b-bee6-a58412b50684"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: batch size is bigger than the data size. Setting batch size to data size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  8.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: batch size is bigger than the data size. Setting batch size to data size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  8.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FID Score (Text-to-Image): 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "R-Precession"
      ],
      "metadata": {
        "id": "JJR4U8Wxu8Dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Load CLIP model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Function to compute R-Precision\n",
        "def compute_r_precision(text_prompts, image_paths, top_k=1):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    text_prompts: List of text prompts\n",
        "    image_paths: List of corresponding generated image paths (including distractor images)\n",
        "    top_k: The number of relevant images to include in the precision calculation.\n",
        "\n",
        "    Returns:\n",
        "    R-Precision score (mean percentage of correct retrievals)\n",
        "    \"\"\"\n",
        "    correct_retrievals = 0\n",
        "    total_samples = len(text_prompts)\n",
        "\n",
        "    for idx, prompt in enumerate(text_prompts):\n",
        "        # Load and preprocess images\n",
        "        images = [preprocess(Image.open(img_path)).unsqueeze(0).to(device) for img_path in image_paths[idx]]\n",
        "        images = torch.cat(images, dim=0)\n",
        "\n",
        "        # Tokenize text prompt\n",
        "        text = clip.tokenize([prompt]).to(device)\n",
        "\n",
        "        # Encode images and text\n",
        "        with torch.no_grad():\n",
        "            image_features = model.encode_image(images)\n",
        "            text_features = model.encode_text(text)\n",
        "\n",
        "        # Normalize features\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Compute cosine similarities\n",
        "        similarities = (image_features @ text_features.T).squeeze()\n",
        "\n",
        "        # Sort by similarity\n",
        "        sorted_indices = torch.argsort(similarities, descending=True)\n",
        "\n",
        "        # Check if the correct image is within the top-k predictions\n",
        "        if 0 in sorted_indices[:top_k]:  # Assuming the first image is the correct one\n",
        "            correct_retrievals += 1\n",
        "\n",
        "    r_precision = correct_retrievals / total_samples\n",
        "    print(f\"R-Precision (Top-{top_k}): {r_precision:.4f}\")\n",
        "    return r_precision\n",
        "\n",
        "# Example Usage:\n",
        "text_prompts = [\n",
        "    \"a photograph of an astronaut riding a horse\",\n",
        "]\n",
        "\n",
        "# List of lists of images (each list should contain one correct image + distractors)\n",
        "image_paths = [\n",
        "    [\"/content/fake2/Unknown.png\", \"/content/real/Unknown-2.png\"]\n",
        "\n",
        "]\n",
        "\n",
        "# Compute R-Precision\n",
        "compute_r_precision(text_prompts, image_paths, top_k=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjVZPaU8rupV",
        "outputId": "e0e1b7ea-2448-4725-fdb0-66a838790afa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-Precision (Top-1): 0.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Diversity Score"
      ],
      "metadata": {
        "id": "nSfABdx1vkZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "# Load CLIP model and preprocessing\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Function to compute Diversity Score\n",
        "def compute_diversity_score(image_paths):\n",
        "    \"\"\"\n",
        "    Compute diversity score (DS) for a set of generated images.\n",
        "\n",
        "    Args:\n",
        "    - image_paths: List of paths to generated images.\n",
        "\n",
        "    Returns:\n",
        "    - diversity_score: The average pairwise cosine distance between image embeddings.\n",
        "    \"\"\"\n",
        "    image_embeddings = []\n",
        "\n",
        "    # Extract embeddings for each image\n",
        "    for img_path in image_paths:\n",
        "        image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            image_embedding = model.encode_image(image)\n",
        "            image_embedding /= image_embedding.norm(dim=-1, keepdim=True)  # Normalize embeddings\n",
        "            image_embeddings.append(image_embedding.cpu().numpy().squeeze())\n",
        "\n",
        "    # Convert list to numpy array\n",
        "    image_embeddings = np.array(image_embeddings)\n",
        "\n",
        "    # Compute pairwise cosine distances\n",
        "    pairwise_distances = []\n",
        "    for (img1, img2) in combinations(image_embeddings, 2):\n",
        "        cosine_similarity = np.dot(img1, img2) / (np.linalg.norm(img1) * np.linalg.norm(img2))\n",
        "        distance = 1 - cosine_similarity  # Cosine distance\n",
        "        pairwise_distances.append(distance)\n",
        "\n",
        "    # Average pairwise distance as diversity score\n",
        "    diversity_score = np.mean(pairwise_distances)\n",
        "    print(f\"Diversity Score (DS): {diversity_score:.4f}\")\n",
        "    return diversity_score\n",
        "\n",
        "# Example Usage\n",
        "generated_image_paths = [\n",
        "    \"/content/fake2/Unknown.png\",\n",
        "    \"/content/real/Unknown-2.png\",\n",
        "]\n",
        "\n",
        "compute_diversity_score(generated_image_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "si7paRgvvkJU",
        "outputId": "f5e3f6b0-bb43-4370-e1c7-f47b8faae2a9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diversity Score (DS): 0.2021\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2021484375"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attentional Consistency Matrix"
      ],
      "metadata": {
        "id": "-MmDNDmSwPXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "\n",
        "# Load CLIP model and preprocessing\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Function to compute attentional consistency\n",
        "def compute_attention_consistency(image_path, text_prompt, sub_phrases):\n",
        "    \"\"\"\n",
        "    Compute attentional consistency between the image and parts of the text prompt.\n",
        "\n",
        "    Args:\n",
        "    - image_path: Path to the generated image.\n",
        "    - text_prompt: The full text prompt.\n",
        "    - sub_phrases: List of sub-phrases (object descriptions, relationships).\n",
        "\n",
        "    Returns:\n",
        "    - consistency_score: Average cosine similarity between sub-phrase embeddings and the full text/image embeddings.\n",
        "    \"\"\"\n",
        "    # Load and preprocess the image\n",
        "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
        "\n",
        "    # Encode full text prompt and image\n",
        "    with torch.no_grad():\n",
        "        text_embedding = model.encode_text(clip.tokenize([text_prompt]).to(device))\n",
        "        image_embedding = model.encode_image(image)\n",
        "        text_embedding /= text_embedding.norm(dim=-1, keepdim=True)\n",
        "        image_embedding /= image_embedding.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Sub-phrase embeddings\n",
        "    sub_phrase_embeddings = []\n",
        "    for phrase in sub_phrases:\n",
        "        with torch.no_grad():\n",
        "            phrase_embedding = model.encode_text(clip.tokenize([phrase]).to(device))\n",
        "            phrase_embedding /= phrase_embedding.norm(dim=-1, keepdim=True)\n",
        "            sub_phrase_embeddings.append(phrase_embedding)\n",
        "\n",
        "    # Compute cosine similarities between sub-phrases and image\n",
        "    consistency_scores = []\n",
        "    for sub_embedding in sub_phrase_embeddings:\n",
        "        similarity = torch.cosine_similarity(sub_embedding, image_embedding).item()\n",
        "        consistency_scores.append(similarity)\n",
        "\n",
        "    # Average consistency score\n",
        "    average_consistency = sum(consistency_scores) / len(consistency_scores)\n",
        "    print(f\"Attentional Consistency Score: {average_consistency:.4f}\")\n",
        "    return average_consistency\n",
        "\n",
        "# Example Usage\n",
        "image_path = \"/content/fake2/Unknown.png\"\n",
        "text_prompt = \"an astronaut riding a horse\"\n",
        "sub_phrases = [\" an astronaut\", \"horse\"]\n",
        "\n",
        "compute_attention_consistency(image_path, text_prompt, sub_phrases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsmgBtpEwKNY",
        "outputId": "615d6342-5e85-4faa-d78c-10b57638551e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attentional Consistency Score: 0.2476\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.24761962890625"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6gEZBIWXvGlG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}